## Introduction

This is simply a report and contains no intrusive commands and can be run, rerun or restarted at any point. The main script is on PJ019A0 in the SDB user home directory and called MAXDBLOGRPT.sh  
<br/>EG: /home/sdb/MAXDBLOGRPT.sh

It should be able to run on any MaxDB or LiveCache system running the MaxDB binaries.

It runs under the control user but only requires that the OS user running the script be the sdb user as it picks up the control user password from the encrypted XUSER password file.

## Crontab entry on PJ019A0

10,25,40,55 \* \* \* \* su - sdb -c "/home/sdb/MAXDBLOGRPT.sh DMP" /dev/null 2>&1

## Purpose

To catch the following conditions and alert and advise the next steps.

- Database size used vs. Database size allocated for percentage of free space.
- Percentage of the log file used
- The autosave status for the log.

This runs through the root crontab on the server.

Logging of results is under the \$HOME/log directory for the SDB user.

This is detailed further in the script:  
<br/>This script is intended to monitor the autolog/autosave parameter as well as the database and log percentage used of a Live Cache or MaxDB database.

This requires only the Database SID as a parameter. We leverage the XUSER to perform the authorizations for the commands. The commands are not invasive and change nothing.  

Actions that need to be taken will be logged and also sent as part of an email. The environment and other diagnostic information is sent back to minimize confusion.

All actions will be logged in a daily file and these will be cleaned up for any timestamp older than \$FILE_EXPIRATION days. It defaults to 35 days.  
<br/>Let's go through the steps of the script in order!  

1\. Verify the usage and produce a help page if the input is missing.  
EG: Usage: sh MAXDBLOGRPT.sh &lt;DB_NAME&gt;  
1a. Initialize the environment and variables.  
Important parameters:

- Log file expiration in days (35)
- mail addresses
- report heading setup.
- Log threshold - at 35%, the alert will be triggered - Autosave normally triggers at 33%.
- Database Size Threshold - Since this is over 5TB, 98% is set here - can be overridden.
- We also check that AUTOSAVE is ON. This should almost always be the case.  
    <br/>**You can override some of these later on near the end of the script.**  

2\. Verify the database is up and running. If not exit cleanly. We need not proceed further.  

3\. Check the autolog/autosave parameter otherwise proceed  

4\. If the autosave/autolog parameter is set to "AUTOSAVE" proceed to log the results  

5\. We also want to record the percentage of the log used and whether the DB is full.  

6\. If there's a problem with the \$LOG_THRESHOLD being greater than 35%, add to an email  

7\. If there's a problem with autolog/autosave being set to off, add to email  

8\. We also want to send the email if the DB is at 100 percent - the alert level will be adjusted depending on overall space  

9\. Housekeeping - clean up old log files.

When there issues, the instructions in the output should help sort out how to proceed.

In the case of AUTOSAVE for the logs, it should always be on if the database is up.

If AUTOSAVE is not on, the usual causes are with a full database, a catastrophic failure or the operating system files are full or missing. (The database will turn it off for data integrity purposes.)

You can also check a couple of scripts for the maximum data size of the database,  
<br/>dbsizehist.sh

logsizehist.sh

These are located in the \$HOME/log directory of the SDB user but these are intended for quickly scraping the logs generated by the script for the database size and the percentage of the MAxDB database log percentage used.

<br/>One of the things I purposely put in the script is a set of overrides for testing. So you can make a copy of this script and test with that copy.

## Troubleshooting issues reported from the MAXDBLOGRPT.sh  

<br/>Logs are located in the SDB user under \$HOME/log and are appended for each day with a new one written daily. It's also cleaned up daily for anything over 35 days by default. This is configurable in the script with the variable: FILE_EXPIRATION = 35

Logs are generally small and can be deleted in a pinch. The files are only about 70K each so you might only save 2.5 MB total.

## Debugging, testing and other actions  

The below is located near the end of the script before the reporting is created.

##########################################

\# EMAIL Overrides! - Testing only, please ensure these are commented out before making live

#############################################

\# For debugging mail and making sure it's sent for testing, activate the below variable values as needed.

\# Otherwise leave commented out as they will send out mails that may be acted upon

\# erroneously. Be sure to change the email address for your own testing. Don't make Kevin mad.

\# MAIL_ALERT_LEVEL="CRITICAL"

\# MAIL_ALERT_LEVEL="WARNING"

\# DATABASEUSED=99

\# DATABASEUSED=100

\# AUTOSAVE_LOG_STATUS="OFF"

\# USED_LOG_SIZE=36

\# TO="<Kevin_fries@colpal.com>"

## Sample email on alert trigger  

<br/>

Fields in RED are there for demonstration purposes and variable.

Note: values are artificial for demo purposes and run on DMP.  
The "DMPBACKUP" field will vary with the &lt;SID&gt; of the database.  
EG: For DMP, the field will be DMPBACKUP. Others are highlighted.  

CRITICAL alert for Database DMP on DMPAPP00

This Email Report is only Generated on Warning or Critical level

A daily physical log of actions can be found at /home/sdb/log/DMP-MAXDBLOGRPT\*.log

Script Name: MAXDBLOGRPT.sh was run at Tue Oct 3 00:16:58 WAUST 2023

CRITICAL alert level found - the following conditions were checked and need attention.

Database Used Percentage = 100

Used Log Size: = 36

AUTOSAVE Log Status = OFF

1\. Autosave of logs enabled: OFF - If this is set to anything but AUTOSAVE, you need to investigate the issue immediately. Possible causes are that the backup directory is full or some other catastrophic event.

To activate AUTOSAVE, run the following ONLY AFTER fixing the cause:

A: Log into the pj019a0 as the user sdb eg: dzdo su - sdb

B: Check the status by: dbmcli -U DMPBACKUP autolog_show

C: turn on AUTOSAVE by the following command:

1\. dbmcli -U DMPBACKUP autolog_on

D: Verify the database state and the AUTOSAVE status after a few minutes via

1\. Check the autosave status = AUTOSAVE

dbmcli -U DMPBACKUP autolog_show

2\. Make sure it's up (status = ONLINE) (ADMIN status will not be sufficient!)

dbmcli -U DMPBACKUP db_state

3\. Check the log percentage used if you want.

dbmcli -U DMPBACKUP info log|grep -i 'Used Size'| grep '%'|awk '{print }'

2\. Database percentage used: 100 - If this is 100 percent you have a serious issue and need to allocate a new datafile. While the monitors should catch this, it's not a bad idea to check if the database needs additional datafiles.

3\. Log percentage used: - If this is over 35 percent, it needs to be investigated.

## Add to another system  

<br/>

Too easy. It's designed to be independent of host and Database SID.  
<br/>1\. Copy the MAXDBLOGRPT.sh from the source to the target home where your LiveCache or MaxDB instance runs. You may want to set the thresholds for  

DATABASE_THRESHOLD=98  
<br/>To different values. For a 20GB DB, a DATABASE_THRESHOLD setting of 98% is probably better but can be reduced to 90 to 95 percent.  
<br/>LOG_THRESHOLD=35  
<br/>For an extremely busy system with logs being backed up at 1/3rd of the size of the log file, you may want to increase it to 50.  
<br/><br/>2\. Copy the dbsizehist.sh and logsizehist.sh from the log directory on an existing instance. Create a log directory with the SDB user on the \$HOME directory of the SDB user and put them there. The scripts are optional but can be used for testing and post mortem tracking.

3\. Set up a cron job if necessary if you want it to repeat. I provided a [crontab example](#_2ardg0zaowb7) earlier and once again, it depends on the frequency you need it to run.15 minutes seems adequate.

: